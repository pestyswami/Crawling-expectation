{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea1bbc9",
   "metadata": {},
   "source": [
    "# 2022년 후보별 기사 수 파악\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd6346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진행중 1\n",
      "진행중 1\n",
      "진행중 1\n",
      "진행중 1\n",
      "종료##########\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "keyworld_list = ['윤석열', '이재명', '안철수', '심상정'] # 검색할 키워드\n",
    "count_list = [] # 전체 기사 수가 저장되는 리스트, count 값들이 저장됨 \n",
    "count = 0 # 해당 일의 기사 수가 저장되는 변수\n",
    "result = [] \n",
    "\n",
    "for i in range (1, 29): # 검색할 기간 지정 ex) 2월의 경우 1, 29\n",
    "    start_day = '2022.02.' + str('{0:02d}'.format(i))\n",
    "    end_day = start_day # 하루 단위로 검색하기에 start_day와 end_day가 동일함\n",
    "    for keyword in keyworld_list:\n",
    "        i = 0\n",
    "        for page in range(1,3991,10): # 네이버기사는 최대 4000페이지까지 검색 가능\n",
    "            url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={keyword}&sort=0&photo=3&field=0&pd=3&ds={start_day}&de={end_day}&cluster_rank=1&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from{start_day}to{end_day},a:all&start={page}\"\n",
    "            res = requests.get(url) # 가져 온 url을 res변수에 담음\n",
    "            res.raise_for_status() # 가져온 HTML에 문제가 있는지 체크, 문제 있을시 종료\n",
    "\n",
    "            soup = BeautifulSoup(res.text, \"lxml\") # res.text를 통해 가져온 HTML 문서를 lxml 파서를 통해서 BeautifulSoup 객체로 만들어 줌\n",
    "            soup_tit = soup.find_all(\"a\", attrs={\"class\": \"news_tit\"}) # \"a\"태그 중 class명이 news_tit 인 애들을 가져옴, 뉴스의 HTML 코드의 class 명은 news_tit\n",
    "\n",
    "            if len(soup_tit) != 10: # 한 페이지에는 10개의 기사 존재\n",
    "                count += len(soup_tit)\n",
    "                break\n",
    "            if (page%100 == 1):\n",
    "                print('진행중', page)\n",
    "            count += 10\n",
    "            \n",
    "        count_list.append(count) # count_list에 해당 일의 기사 수 추가\n",
    "        count = 0\n",
    "        i += 1\n",
    "    \n",
    "\n",
    "    day = start_day\n",
    "    Yoon = count_list[0]\n",
    "    Lee = count_list[1]\n",
    "    Ahn = count_list[2]\n",
    "    Shim = count_list[3]\n",
    "    result.append([day, Yoon, Lee, Ahn, Shim])\n",
    "    count = 0\n",
    "    count_list = []\n",
    "\n",
    "bigdata_table = pd.DataFrame(result, columns=('day','윤석열','이재명','안철수', '심상정')) # 데이터 프레임 생성\n",
    "bigdata_table.to_csv(\"news22_02.csv\", encoding=\"utf-8\", mode ='w' ,index = True) # csv 파일로 저장\n",
    "print(\"종료##########\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b51ba",
   "metadata": {},
   "source": [
    "# 대통령 기사 수 파악\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3478c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진행중 1\n",
      "진행중 101\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 1\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 501\n",
      "진행중 601\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 501\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 1\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 1\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 401\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 1\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "진행중 301\n",
      "진행중 1\n",
      "진행중 101\n",
      "진행중 201\n",
      "종료##########\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "keyworld_list = ['윤석열'] # 검색할 키워드\n",
    "count_list = [] # 전체 기사 수가 저장되는 리스트, count 값들이 저장됨 \n",
    "count = 0 # 해당 일의 기사 수가 저장되는 변수\n",
    "result = [] \n",
    "\n",
    "for i in range (1, 32): # 검색할 기간 지정 ex) 5월의 경우 1, 32\n",
    "    start_day = '2022.05.' + str('{0:02d}'.format(i))\n",
    "    end_day = start_day # 하루 단위로 검색하기에 start_day와 end_day가 동일함\n",
    "    for keyword in keyworld_list:\n",
    "        i = 0\n",
    "        for page in range(1,3991,10): # 네이버기사는 최대 4000페이지까지 검색 가능\n",
    "            url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={keyword}&sort=0&photo=3&field=0&pd=3&ds={start_day}&de={end_day}&cluster_rank=1&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from{start_day}to{end_day},a:all&start={page}\"\n",
    "            res = requests.get(url) # 가져 온 url을 res변수에 담음\n",
    "            res.raise_for_status() # 가져온 HTML에 문제가 있는지 체크, 문제 있을시 종료\n",
    "\n",
    "            soup = BeautifulSoup(res.text, \"lxml\") # res.text를 통해 가져온 HTML 문서를 lxml 파서를 통해서 BeautifulSoup 객체로 만들어 줌\n",
    "            soup_tit = soup.find_all(\"a\", attrs={\"class\": \"news_tit\"}) # \"a\"태그 중 class명이 news_tit 인 애들을 가져옴, 뉴스의 HTML 코드의 class 명은 news_tit\n",
    "\n",
    "            if len(soup_tit) != 10: # 한 페이지에는 10개의 기사 존재\n",
    "                count += len(soup_tit)\n",
    "                break\n",
    "            if (page%100 == 1):\n",
    "                print('진행중', page)\n",
    "            count += 10\n",
    "            \n",
    "        count_list.append(count) # count_list에 해당 일의 기사 수 추가\n",
    "        count = 0\n",
    "        i += 1\n",
    "    \n",
    "\n",
    "    day = start_day\n",
    "    Yoon = count_list[0]\n",
    "    result.append([day, Yoon])\n",
    "    count = 0\n",
    "    count_list = []\n",
    "\n",
    "bigdata_table = pd.DataFrame(result, columns=('day','윤석열')) # 데이터 프레임 생성\n",
    "bigdata_table.to_csv(\"yoon_05.csv\", encoding=\"utf-8\", mode ='w' ,index = True) # csv 파일로 저장\n",
    "print(\"종료##########\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
